---
title: "Epochs & Learning Patterns: How AI Develops Intelligence Over Time"
description: "Dive deep into epochs, learning curves, and how neural networks develop pattern recognition through iterative training"
type: "Exercise"
difficulty: "Intermediate"
duration: "75 minutes"
language: "Python"
category: "Fundamentals"
exercise_type: "Code Analysis & Implementation"
prerequisites: ["Neural Network Anatomy (Exercise 1)", "Basic understanding of loops", "NumPy basics"]
learning_objectives:
  - "Master the concept of epochs and why repetition builds intelligence"
  - "Analyze learning curves and understand overfitting vs underfitting"
  - "Implement different learning rate strategies"
  - "Visualize how patterns emerge through repeated training"
  - "Build intuition for training convergence and stopping criteria"
tools_needed: ["Python 3.8+", "NumPy", "Matplotlib", "Code editor"]
estimated_completion: "75-90 minutes"
difficulty_score: 7
tags: ["epochs", "learning-curves", "pattern-recognition", "optimization", "convergence"]
series: "Python AI Training Fundamentals"
series_part: 2
whiteboard_required: true
code_template_provided: true
solution_provided: true
auto_grading: false
peer_review: true
instructor_review: true
image: "https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=3540&auto=format&fit=crop"
---

# Epochs & Learning Patterns: Building AI Intelligence ğŸ§ âš¡

> *"An epoch is like a day in school - each day the student sees all the lessons again, but gets a little smarter each time."*

## ğŸ¯ Exercise Overview

In this exercise, you'll explore how neural networks learn through repeated exposure to data. You'll implement sophisticated training loops, analyze learning curves, and understand when your AI has learned enough (or too much!).

### Key Concepts We'll Master

```
ğŸ“š One Epoch = One complete pass through ALL training data
ğŸ”„ Multiple Epochs = Repeated learning, building stronger patterns
ğŸ“ˆ Learning Curves = Visual story of AI getting smarter
ğŸ¯ Convergence = When the AI has learned as much as it can
âš ï¸ Overfitting = When the AI memorizes instead of learning
```

---

## ğŸ”¬ Part 1: Understanding Epochs Through Visual Learning

Let's start by building an enhanced training system that shows exactly how learning progresses:

<InteractiveCodeEditor template={`
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

class EpochVisualizer:
    def __init__(self):
        self.epoch_data = []
        self.pattern_strength = defaultdict(list)
        
    def record_epoch(self, epoch, loss, accuracy, pattern_predictions):
        """Record what happens in each epoch"""
        self.epoch_data.append({
            'epoch': epoch,
            'loss': loss,
            'accuracy': accuracy,
            'predictions': pattern_predictions.copy()
        })
        
        # Track how well specific patterns are learned
        for pattern, confidence in pattern_predictions.items():
            self.pattern_strength[pattern].append(confidence)
    
    def show_learning_progress(self):
        """Visualize how the AI learns over time"""
        if not self.epoch_data:
            print("No data to visualize yet!")
            return
            
        epochs = [d['epoch'] for d in self.epoch_data]
        losses = [d['loss'] for d in self.epoch_data]
        accuracies = [d['accuracy'] for d in self.epoch_data]
        
        print("ğŸ“Š LEARNING PROGRESS ANALYSIS")
        print("=" * 50)
        
        # Show numerical progress
        for i, data in enumerate(self.epoch_data[::10]):  # Show every 10th epoch
            print(f"Epoch {data['epoch']:3d}: Loss={data['loss']:.4f}, Accuracy={data['accuracy']:.1f}%")
        
        # Analyze learning phases
        if len(losses) > 20:
            early_loss = np.mean(losses[:10])
            late_loss = np.mean(losses[-10:])
            improvement = early_loss - late_loss
            
            print(f"\\nğŸš€ LEARNING ANALYSIS:")
            print(f"   Early training loss: {early_loss:.4f}")
            print(f"   Late training loss: {late_loss:.4f}")
            print(f"   Total improvement: {improvement:.4f}")
            
            if improvement > 0.5:
                print("   ğŸ“ˆ Excellent learning progress!")
            elif improvement > 0.1:
                print("   ğŸ˜Š Good learning progress")
            elif improvement > 0.01:
                print("   ğŸ˜ Slow but steady progress")
            else:
                print("   âš ï¸ Learning has plateaued")

# Enhanced vocabulary for more complex patterns
vocabulary = {
    "the": 1, "cat": 2, "dog": 3, "sat": 4, "ran": 5, "on": 6, "in": 7,
    "mat": 8, "park": 9, "house": 10, "big": 11, "small": 12, "red": 13, 
    "blue": 14, "quickly": 15, "slowly": 16, "jumped": 17, "over": 18, "under": 19
}

id_to_word = {v: k for k, v in vocabulary.items()}

# More complex training data with patterns to discover
training_sentences = [
    "the big cat sat on the red mat",
    "the small dog ran in the blue park",
    "the big dog jumped over the small cat",
    "the cat ran quickly under the big house",
    "the small cat sat on the blue mat",
    "the big dog ran slowly in the red park",
    "the cat jumped over the small house",
    "the dog sat under the big mat"
]

print("ğŸ§  Enhanced Vocabulary:", len(vocabulary), "words")
print("ğŸ“š Training Sentences:", len(training_sentences))
for i, sentence in enumerate(training_sentences, 1):
    print(f"  {i}. {sentence}")
`} language="python" />

---

## ğŸ”„ Part 2: The Epoch Training Engine

Now let's build a sophisticated training engine that tracks learning across epochs:

<InteractiveCodeEditor template={`
class EpochTrainingEngine:
    def __init__(self, neural_network, learning_rate=0.1):
        self.nn = neural_network
        self.learning_rate = learning_rate
        self.visualizer = EpochVisualizer()
        
        # Track different metrics across epochs
        self.loss_history = []
        self.accuracy_history = []
        self.learning_rate_history = []
        self.epoch_times = []
        
    def prepare_training_data(self, sentences):
        """Convert sentences into input-output pairs with more sophistication"""
        X_train, y_train, contexts = [], [], []
        
        for sentence in sentences:
            words = sentence.split()
            for i in range(len(words) - 1):
                # Use last 3 words as context (or all if fewer than 3)
                context_start = max(0, i - 2)
                context = words[context_start:i+1]
                target = words[i+1]
                
                # Convert to one-hot encoding
                x = np.zeros(self.nn.vocab_size)
                for word in context:
                    if word in vocabulary:
                        x[vocabulary[word] - 1] = 1
                
                if target in vocabulary:
                    X_train.append(x)
                    y_train.append(vocabulary[target] - 1)
                    contexts.append(" ".join(context) + " â†’")
        
        return np.array(X_train), np.array(y_train), contexts
    
    def calculate_accuracy(self, X, y):
        """Calculate how often we predict the right word"""
        predictions = self.nn.forward(X)
        predicted_classes = np.argmax(predictions, axis=1)
        accuracy = np.mean(predicted_classes == y) * 100
        return accuracy
    
    def test_pattern_recognition(self):
        """Test how well the AI recognizes specific patterns"""
        pattern_tests = {
            "the big": ["cat", "dog", "house"],
            "the small": ["cat", "dog", "house"], 
            "sat on": ["the", "mat"],
            "ran in": ["the", "park"],
            "jumped over": ["the", "cat", "house"]
        }
        
        pattern_scores = {}
        for context, possible_words in pattern_tests.items():
            context_words = context.split()
            predicted_word, confidence = self.nn.predict_next_word(context_words)
            
            # Score based on whether prediction makes sense
            if predicted_word in possible_words:
                pattern_scores[context] = confidence
            else:
                pattern_scores[context] = 0.0
                
        return pattern_scores
    
    def train_with_epochs(self, sentences, epochs=100, verbose=True):
        """Train for multiple epochs with detailed tracking"""
        print("ğŸš€ EPOCH TRAINING STARTED!")
        print("=" * 60)
        
        X_train, y_train, contexts = self.prepare_training_data(sentences)
        print(f"ğŸ“Š Training examples: {len(X_train)}")
        print(f"ğŸ¯ Unique patterns to learn: {len(set(contexts))}")
        
        for epoch in range(epochs):
            # One epoch = one pass through ALL training data
            epoch_loss = 0
            
            # Shuffle data for better learning (important!)
            indices = np.random.permutation(len(X_train))
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            # Forward and backward pass for all data
            predictions = self.nn.forward(X_shuffled)
            
            # Calculate loss for this epoch
            for i in range(len(y_shuffled)):
                epoch_loss += -np.log(predictions[i, y_shuffled[i]] + 1e-15)
            epoch_loss /= len(y_shuffled)
            
            # Backward pass (gradient descent)
            self._backward_pass(X_shuffled, y_shuffled, predictions)
            
            # Calculate accuracy
            accuracy = self.calculate_accuracy(X_train, y_train)
            
            # Test pattern recognition
            pattern_scores = self.test_pattern_recognition()
            
            # Record epoch data
            self.loss_history.append(epoch_loss)
            self.accuracy_history.append(accuracy)
            self.learning_rate_history.append(self.learning_rate)
            self.visualizer.record_epoch(epoch, epoch_loss, accuracy, pattern_scores)
            
            # Show progress
            if verbose and (epoch % 20 == 0 or epoch < 5):
                print(f"Epoch {epoch:3d}: Loss={epoch_loss:.4f}, Accuracy={accuracy:.1f}%")
                
                # Show best learned pattern
                best_pattern = max(pattern_scores.keys(), key=lambda k: pattern_scores[k])
                best_score = pattern_scores[best_pattern]
                print(f"          Best pattern: '{best_pattern}' ({best_score*100:.1f}% confidence)")
                
                # Adaptive learning rate
                if epoch > 0 and self.loss_history[-1] > self.loss_history[-2]:
                    self.learning_rate *= 0.95  # Reduce if loss increased
                    print(f"          Learning rate adjusted to: {self.learning_rate:.4f}")
        
        print(f"\\nğŸ‰ TRAINING COMPLETED!")
        print(f"   Final loss: {self.loss_history[-1]:.4f}")
        print(f"   Final accuracy: {self.accuracy_history[-1]:.1f}%")
        print(f"   Total epochs: {epochs}")
        
        return self.loss_history, self.accuracy_history
    
    def _backward_pass(self, X, y, predictions):
        """Simplified backward pass implementation"""
        batch_size = X.shape[0]
        
        # Output layer gradients
        d_output = predictions.copy()
        for i in range(batch_size):
            d_output[i, y[i]] -= 1
        d_output /= batch_size
        
        # Hidden layer gradients
        d_W2 = self.nn.a1.T @ d_output
        d_b2 = np.sum(d_output, axis=0, keepdims=True)
        
        d_hidden = d_output @ self.nn.W2.T
        d_hidden = d_hidden * self.nn.a1 * (1 - self.nn.a1)
        
        d_W1 = X.T @ d_hidden
        d_b1 = np.sum(d_hidden, axis=0, keepdims=True)
        
        # Update weights
        self.nn.W2 -= self.learning_rate * d_W2
        self.nn.b2 -= self.learning_rate * d_b2
        self.nn.W1 -= self.learning_rate * d_W1
        self.nn.b1 -= self.learning_rate * d_b1

# Simple neural network (reusing from previous exercise)
class SimpleNeuralNetwork:
    def __init__(self, vocab_size, hidden_size):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.W1 = np.random.randn(vocab_size, hidden_size) * 0.1
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, vocab_size) * 0.1
        self.b2 = np.zeros((1, vocab_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        self.z1 = X @ self.W1 + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = self.softmax(self.z2)
        return self.a2
    
    def predict_next_word(self, context_words):
        X = np.zeros((1, self.vocab_size))
        for word in context_words:
            if word in vocabulary:
                X[0, vocabulary[word] - 1] = 1
        
        probabilities = self.forward(X)
        predicted_id = np.argmax(probabilities) + 1
        predicted_word = id_to_word.get(predicted_id, "unknown")
        confidence = probabilities[0, predicted_id - 1]
        return predicted_word, confidence

# Create and train a more sophisticated model
brain = SimpleNeuralNetwork(len(vocabulary), hidden_size=15)
trainer = EpochTrainingEngine(brain, learning_rate=0.3)

print("ğŸ§  Enhanced brain ready for epoch training!")
`} language="python" />

---

## ğŸ“ˆ Part 3: The Learning Curve Analysis Drill

Now let's run the training and analyze how learning progresses:

<InteractiveCodeEditor template={`
# ğŸš€ EPOCH TRAINING DRILL
print("Starting epoch-based training...")
loss_history, accuracy_history = trainer.train_with_epochs(
    training_sentences, 
    epochs=150, 
    verbose=True
)

# ğŸ“Š LEARNING CURVE ANALYSIS
print("\\n" + "="*60)
print("ğŸ“ˆ LEARNING CURVE ANALYSIS")
print("="*60)

def analyze_learning_phases(losses, accuracies):
    """Analyze different phases of learning"""
    total_epochs = len(losses)
    
    # Divide training into phases
    phase_size = total_epochs // 3
    early_phase = losses[:phase_size]
    middle_phase = losses[phase_size:2*phase_size]
    late_phase = losses[2*phase_size:]
    
    print(f"ğŸ“Š Training divided into 3 phases of ~{phase_size} epochs each:")
    print()
    
    # Early phase analysis
    early_loss_avg = np.mean(early_phase)
    early_loss_trend = early_phase[-1] - early_phase[0]
    print(f"ğŸŒ± EARLY PHASE (Epochs 0-{phase_size}):")
    print(f"   Average loss: {early_loss_avg:.4f}")
    print(f"   Loss change: {early_loss_trend:.4f}")
    if early_loss_trend < -0.5:
        print("   Status: ğŸš€ Rapid learning! Brain is figuring out patterns")
    elif early_loss_trend < -0.1:
        print("   Status: ğŸ“ˆ Good progress, patterns emerging")
    else:
        print("   Status: ğŸ˜ Slow start, needs more time")
    print()
    
    # Middle phase analysis
    middle_loss_avg = np.mean(middle_phase)
    middle_loss_trend = middle_phase[-1] - middle_phase[0]
    print(f"âš¡ MIDDLE PHASE (Epochs {phase_size}-{2*phase_size}):")
    print(f"   Average loss: {middle_loss_avg:.4f}")
    print(f"   Loss change: {middle_loss_trend:.4f}")
    if middle_loss_trend < -0.2:
        print("   Status: ğŸ’ª Strong learning continues")
    elif middle_loss_trend < -0.05:
        print("   Status: ğŸ“Š Steady improvement")
    else:
        print("   Status: ğŸ¯ Learning stabilizing")
    print()
    
    # Late phase analysis
    late_loss_avg = np.mean(late_phase)
    late_loss_trend = late_phase[-1] - late_phase[0]
    print(f"ğŸ¯ LATE PHASE (Epochs {2*phase_size}-{total_epochs}):")
    print(f"   Average loss: {late_loss_avg:.4f}")
    print(f"   Loss change: {late_loss_trend:.4f}")
    if late_loss_trend > 0.01:
        print("   Status: âš ï¸ Possible overfitting - loss increasing!")
    elif abs(late_loss_trend) < 0.01:
        print("   Status: ğŸ Converged - optimal learning achieved")
    else:
        print("   Status: ğŸ“‰ Still improving slowly")
    print()
    
    # Overall assessment
    total_improvement = losses[0] - losses[-1]
    print(f"ğŸ–ï¸ OVERALL TRAINING ASSESSMENT:")
    print(f"   Total loss improvement: {total_improvement:.4f}")
    print(f"   Final accuracy: {accuracies[-1]:.1f}%")
    
    if total_improvement > 1.0 and accuracies[-1] > 70:
        print("   Grade: ğŸ† Excellent training!")
    elif total_improvement > 0.5 and accuracies[-1] > 50:
        print("   Grade: ğŸ˜Š Good training!")
    elif total_improvement > 0.1:
        print("   Grade: ğŸ˜ Adequate training")
    else:
        print("   Grade: ğŸ˜Ÿ Poor training - needs adjustment")

# Run the analysis
analyze_learning_phases(loss_history, accuracy_history)

# Show learning progress visualization
trainer.visualizer.show_learning_progress()
`} language="python" />

---

## ğŸ¯ Part 4: Advanced Pattern Recognition Drill

Let's test how well our AI learned different types of patterns:

<InteractiveCodeEditor template={`
def advanced_pattern_testing():
    """Test sophisticated pattern recognition capabilities"""
    print("ğŸ§ª ADVANCED PATTERN RECOGNITION TEST")
    print("=" * 50)
    
    # Test different types of patterns
    pattern_categories = {
        "Adjective + Noun": [
            (["big"], "Should predict objects that can be big"),
            (["small"], "Should predict objects that can be small"),
            (["red"], "Should predict objects that can be red"),
            (["blue"], "Should predict objects that can be blue")
        ],
        
        "Subject + Action": [
            (["the", "cat"], "Should predict cat actions"),
            (["the", "dog"], "Should predict dog actions"),
            (["cat"], "Should predict what cats do"),
            (["dog"], "Should predict what dogs do")
        ],
        
        "Action + Preposition": [
            (["sat", "on"], "Should predict what things sit on"),
            (["ran", "in"], "Should predict where things run"),
            (["jumped", "over"], "Should predict what gets jumped over")
        ],
        
        "Complex Context": [
            (["the", "big", "cat"], "Should understand size + animal context"),
            (["ran", "quickly", "in"], "Should understand action + manner + location"),
            (["jumped", "over", "the"], "Should predict what comes after 'the'")
        ]
    }
    
    all_scores = []
    
    for category, tests in pattern_categories.items():
        print(f"\\nğŸ“‹ {category} Patterns:")
        category_scores = []
        
        for context, description in tests:
            predicted_word, confidence = brain.predict_next_word(context)
            category_scores.append(confidence)
            all_scores.append(confidence)
            
            print(f"   '{' '.join(context)}' â†’ '{predicted_word}' ({confidence*100:.1f}%)")
            print(f"      {description}")
            
            # Provide learning insight
            if confidence > 0.7:
                print("      âœ… Strong pattern recognition!")
            elif confidence > 0.4:
                print("      ğŸ˜Š Good pattern understanding")
            elif confidence > 0.2:
                print("      ğŸ˜ Weak pattern detection")
            else:
                print("      âŒ Pattern not learned well")
        
        avg_score = np.mean(category_scores)
        print(f"   ğŸ“Š Category average: {avg_score*100:.1f}%")
    
    # Overall pattern recognition assessment
    overall_avg = np.mean(all_scores)
    print(f"\\nğŸ–ï¸ OVERALL PATTERN RECOGNITION SCORE: {overall_avg*100:.1f}%")
    
    if overall_avg > 0.6:
        print("ğŸ† Excellent pattern recognition! AI shows strong understanding.")
    elif overall_avg > 0.4:
        print("ğŸ˜Š Good pattern recognition. AI is learning well.")
    elif overall_avg > 0.2:
        print("ğŸ˜ Basic pattern recognition. Needs more training.")
    else:
        print("ğŸ˜Ÿ Poor pattern recognition. Major improvements needed.")
    
    return overall_avg

# Run advanced pattern testing
pattern_score = advanced_pattern_testing()
`} language="python" />

---

## ğŸ”„ Part 5: The Overfitting vs Underfitting Drill

Now let's understand when the AI learns too little or too much:

<InteractiveCodeEditor template={`
def overfitting_analysis():
    """Analyze if our model is overfitting, underfitting, or just right"""
    print("ğŸ” OVERFITTING vs UNDERFITTING ANALYSIS")
    print("=" * 50)
    
    # Create separate test data (unseen during training)
    test_sentences = [
        "the big cat jumped quickly",
        "the small dog sat slowly", 
        "the red house stood big",
        "the blue mat lay small"
    ]
    
    print("ğŸ“Š Test Data (Never seen during training):")
    for sentence in test_sentences:
        print(f"   '{sentence}'")
    
    # Test on training data vs test data
    def test_on_dataset(sentences, dataset_name):
        trainer_test = EpochTrainingEngine(brain, learning_rate=0.1)
        X_test, y_test, _ = trainer_test.prepare_training_data(sentences)
        
        if len(X_test) == 0:
            print(f"   {dataset_name}: No valid test cases")
            return 0
            
        accuracy = trainer_test.calculate_accuracy(X_test, y_test)
        
        # Calculate average confidence
        total_confidence = 0
        count = 0
        for sentence in sentences:
            words = sentence.split()
            for i in range(len(words) - 2):
                context = words[i:i+2]
                predicted_word, confidence = brain.predict_next_word(context)
                total_confidence += confidence
                count += 1
        
        avg_confidence = total_confidence / count if count > 0 else 0
        
        print(f"   {dataset_name}:")
        print(f"      Accuracy: {accuracy:.1f}%")
        print(f"      Avg Confidence: {avg_confidence*100:.1f}%")
        
        return accuracy, avg_confidence
    
    print("\\nğŸ¯ Performance Comparison:")
    train_acc, train_conf = test_on_dataset(training_sentences, "Training Data")
    test_acc, test_conf = test_on_dataset(test_sentences, "Test Data")
    
    # Analyze the results
    print("\\nğŸ§  LEARNING ASSESSMENT:")
    
    accuracy_gap = train_acc - test_acc
    confidence_gap = train_conf - test_conf
    
    print(f"   Accuracy gap: {accuracy_gap:.1f}%")
    print(f"   Confidence gap: {confidence_gap*100:.1f}%")
    
    if accuracy_gap > 20 or confidence_gap > 0.3:
        print("   âš ï¸ OVERFITTING DETECTED!")
        print("      Model memorized training data but can't generalize")
        print("      Solutions: More training data, less epochs, regularization")
    elif train_acc < 40:
        print("   ğŸ˜Ÿ UNDERFITTING DETECTED!")
        print("      Model hasn't learned enough from training data")
        print("      Solutions: More epochs, larger network, better features")
    else:
        print("   âœ… GOOD FIT!")
        print("      Model learned patterns without memorizing")
        print("      This is the ideal learning state!")
    
    # Epoch analysis for overfitting detection
    print("\\nğŸ“ˆ EPOCH PROGRESSION ANALYSIS:")
    recent_losses = loss_history[-20:]  # Last 20 epochs
    if len(recent_losses) >= 10:
        early_recent = np.mean(recent_losses[:10])
        late_recent = np.mean(recent_losses[-10:])
        
        if late_recent > early_recent:
            print("   âš ï¸ Loss increased in recent epochs - possible overfitting!")
            print("   Consider early stopping or reducing learning rate")
        elif abs(late_recent - early_recent) < 0.001:
            print("   ğŸ¯ Loss plateaued - model has converged")
            print("   Training can be stopped")
        else:
            print("   ğŸ“ˆ Loss still decreasing - can continue training")

overfitting_analysis()
`} language="python" />

---

## ğŸ¨ Whiteboard Exercise: Mapping the Learning Journey

**Instructions for Whiteboard:**

1. **Draw the Learning Curve**:
   - X-axis: Epochs (0 to 150)
   - Y-axis: Loss (high to low)
   - Draw the characteristic learning curve shape
   - Mark the three phases: Rapid Learning, Steady Improvement, Convergence

2. **Illustrate Overfitting**:
   - Draw two curves: Training Loss vs Validation Loss
   - Show the point where validation loss starts increasing
   - Mark the "optimal stopping point"

3. **Pattern Emergence Map**:
   - Draw how patterns strengthen over epochs
   - Show simple patterns learned first (e.g., "the" â†’ "cat")
   - Show complex patterns learned later (e.g., "big cat" â†’ "jumped")

---

## ğŸ† Final Challenge: Design Your Own Learning Experiment

<InteractiveCodeEditor template={`
# ğŸ† ADVANCED CHALLENGE: Design Your Own Learning Experiment

def learning_experiment_designer():
    """Design and run your own learning experiment"""
    
    print("ğŸ”¬ DESIGN YOUR LEARNING EXPERIMENT")
    print("=" * 50)
    
    # TODO: Design your own vocabulary and training data
    # Focus on a specific domain (e.g., cooking, sports, weather)
    experimental_vocab = {
        "chef": 1, "cooks": 2, "delicious": 3, "food": 4, "kitchen": 5,
        # ADD YOUR WORDS HERE
    }
    
    experimental_sentences = [
        "chef cooks delicious food",
        # ADD YOUR SENTENCES HERE
    ]
    
    # TODO: Experiment with different learning rates
    learning_rates = [0.1, 0.3, 0.5, 0.8]
    
    print("ğŸ§ª YOUR EXPERIMENTAL DESIGN:")
    print(f"   Domain: {list(experimental_vocab.keys())[:5]}...")
    print(f"   Training sentences: {len(experimental_sentences)}")
    print(f"   Learning rates to test: {learning_rates}")
    
    # TODO: Run experiments with different configurations
    results = {}
    
    for lr in learning_rates:
        print(f"\\nğŸš€ Testing learning rate: {lr}")
        
        # Create fresh brain for each experiment
        exp_brain = SimpleNeuralNetwork(len(experimental_vocab), hidden_size=10)
        exp_trainer = EpochTrainingEngine(exp_brain, learning_rate=lr)
        
        # Train for fewer epochs to see differences
        loss_hist, acc_hist = exp_trainer.train_with_epochs(
            experimental_sentences, epochs=50, verbose=False
        )
        
        final_loss = loss_hist[-1]
        final_accuracy = acc_hist[-1]
        
        results[lr] = {
            'final_loss': final_loss,
            'final_accuracy': final_accuracy,
            'convergence_epoch': len([l for l in loss_hist if abs(l - final_loss) > 0.01])
        }
        
        print(f"   Final loss: {final_loss:.4f}")
        print(f"   Final accuracy: {final_accuracy:.1f}%")
    
    # Analyze results
    print("\\nğŸ“Š EXPERIMENTAL RESULTS:")
    best_lr = min(results.keys(), key=lambda k: results[k]['final_loss'])
    print(f"   Best learning rate: {best_lr}")
    print(f"   Best final loss: {results[best_lr]['final_loss']:.4f}")
    
    print("\\nğŸ¯ YOUR CHALLENGE:")
    print("   1. Complete the experimental_vocab with 15+ words")
    print("   2. Create 8+ training sentences")
    print("   3. Test different hidden layer sizes (5, 10, 20)")
    print("   4. Find the optimal training configuration")
    print("   5. Document what patterns your AI learned best")
    
    return results

# Run the experiment designer
experiment_results = learning_experiment_designer()
`} language="python" />

---

## ğŸ¯ Key Concepts Mastered

âœ… **Epochs**: Complete passes through training data build stronger intelligence  
âœ… **Learning Curves**: Visual representation of AI getting smarter over time  
âœ… **Pattern Recognition**: How complex patterns emerge through repetition  
âœ… **Convergence**: Recognizing when optimal learning is achieved  
âœ… **Overfitting vs Underfitting**: Balancing memorization vs generalization  
âœ… **Learning Rate Impact**: How speed of learning affects final performance  

## ğŸš€ What's Next?

You've mastered the art of epochs and learning progression! Next exercises will cover:
- **Advanced Optimization Techniques** (Exercise 3)
- **Real-world Text Processing** (Exercise 4)
- **Testing & Validation Strategies** (Exercise 5)
- **Production AI Deployment** (Exercise 6)

<ExerciseAssessment />

---

*"Intelligence is not fixed. It grows through practice, repetition, and the courage to make mistakes and learn from them." - The AI Training Philosophy* 